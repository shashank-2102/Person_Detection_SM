{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/shashank/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-5-20 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (Quadro T1000 with Max-Q Design, 3912MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /home/shashank/.local/lib/python3.8/site-packages/requirements.txt not found, check failed.\n",
      "\n",
      " \n",
      "Device Used (if its not cuda gl) cuda\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'VideoCapture'\n> Overload resolution failed:\n>  - Can't convert object to 'str' for 'filename'\n>  - VideoCapture() missing required argument 'apiPreference' (pos 2)\n>  - Argument 'index' is required to be an integer\n>  - VideoCapture() missing required argument 'apiPreference' (pos 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 199\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39m#create new obj and execute\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m#give video url and output file name\u001b[39;00m\n\u001b[1;32m    198\u001b[0m detection \u001b[39m=\u001b[39m ObjectDetection(\u001b[39m\"\u001b[39m\u001b[39m/home/shashank/Downloads/outdoor_day1_data.bag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRosbag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvideo_t7.avi\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m detection()\n\u001b[1;32m    200\u001b[0m \u001b[39m#choose between 'Local', 'Webcam' and 'YT' for input\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m#either give URL or path for YT and Local respectively\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 159\u001b[0m, in \u001b[0;36mObjectDetection.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    153\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m    This function is called when the class is executed. Runs loop to read video frame by frame and outputs the result to a new file\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    :return: void\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     player \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_video_from_url()\n\u001b[1;32m    160\u001b[0m     \u001b[39massert\u001b[39;00m player\u001b[39m.\u001b[39misOpened()\n\u001b[1;32m    161\u001b[0m     x_shape \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(player\u001b[39m.\u001b[39mget(cv2\u001b[39m.\u001b[39mCAP_PROP_FRAME_WIDTH))\n",
      "Cell \u001b[0;32mIn[10], line 81\u001b[0m, in \u001b[0;36mObjectDetection.get_video_from_url\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m bag \u001b[39m=\u001b[39m rosbag\u001b[39m.\u001b[39mBag(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_URL)\n\u001b[1;32m     80\u001b[0m bridge \u001b[39m=\u001b[39m CvBridge()\n\u001b[0;32m---> 81\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mVideoCapture(bag\u001b[39m.\u001b[39;49mget_type_and_topic_info(\u001b[39m'\u001b[39;49m\u001b[39m/davis/left/image_raw\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'VideoCapture'\n> Overload resolution failed:\n>  - Can't convert object to 'str' for 'filename'\n>  - VideoCapture() missing required argument 'apiPreference' (pos 2)\n>  - Argument 'index' is required to be an integer\n>  - VideoCapture() missing required argument 'apiPreference' (pos 2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pafy #take videos from yt and pass to model\n",
    "from time import time\n",
    "import rosbag\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "\n",
    "#note: base code developed using: https://www.youtube.com/watch?v=3wdqO_vYMpA&t=0s\n",
    "\n",
    "class ObjectDetection:\n",
    "    \"\"\"\n",
    "    Implements the YOLO V5 Model on a YT video, webcam or local file using OpenCV \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url, inp_typ, out_file):\n",
    "        \"\"\"\n",
    "        Initialises the class with the YT Url and the Output File\n",
    "        :param url: A valid YT URL OR Local file location\n",
    "        :paral inp_typ: User defined either 'Webcam', 'Local' or 'YT'\n",
    "        :out_file: A valid output file name.\n",
    "        :r type: None\n",
    "        \"\"\"\n",
    "        #initilising attributes\n",
    "        self.input_t = inp_typ \n",
    "        self._URL = url  #can be YT url or local file path\n",
    "        self.model = self.load_model()\n",
    "        self.classes = self.model.names\n",
    "        self.out_file = out_file\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu :(' #checks if cuda is available and uses it if it is\n",
    "        print(\"\\n \\nDevice Used (if its not cuda gl)\", self.device)\n",
    "\n",
    "    def get_video_from_url(self):\n",
    "        \"\"\"\n",
    "        Generates video streaming object. Frame by frame extraction will be done in order to make predictions\n",
    "        :return: openCV2 video capture object, with lowest quality frame available for video\n",
    "        \"\"\"\n",
    "        #distingish between local file and YT input\n",
    "        \n",
    "        \n",
    "        if self.input_t == \"Webcam\":\n",
    "            print(\"Opening Webcam\")\n",
    "            cap = cv2.VideoCapture(0)\n",
    "            # Set resolution of input frames to 640x480\n",
    "            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "            # Set frame rate of input frames to 30 frames per second\n",
    "            cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "            return cap\n",
    "        \n",
    "        elif self.input_t == \"Local\":\n",
    "            print(\"Loading local video file\")\n",
    "            input_file = self._URL #test for mp4\n",
    "            cap = cv2.VideoCapture(input_file)\n",
    "            # Set resolution of input frames to 640x480\n",
    "            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) #doesnt seem to work yet, check\n",
    "\n",
    "            # Set frame rate of input rames to 30 frames per second\n",
    "            cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "            return cap\n",
    "\n",
    "        elif self.input_t == \"YT\":\n",
    "            print(\"Loading YT Video\")\n",
    "            play = pafy.new(self._URL).streams[-1]\n",
    "            input_file = play.url\n",
    "            cap = cv2.VideoCapture(input_file)\n",
    "            # Set resolution of input frames to 640x480\n",
    "            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 5)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 5)\n",
    "\n",
    "            # Set frame rate of input frames to 30 frames per second\n",
    "            cap.set(cv2.CAP_PROP_FPS, 100)\n",
    "            return cap\n",
    "        \n",
    "        elif self.input_t == \"Rosbag\":\n",
    "            bag = rosbag.Bag(self._URL)\n",
    "            bridge = CvBridge()\n",
    "            cap = cv2.VideoCapture(bag.get_type_and_topic_info('/davis/left/image_raw'))\n",
    "        \n",
    "\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads YOLO V5 Model from PyTorch\n",
    "        :return: Train model from PyTorch\n",
    "        \"\"\"\n",
    "        #you can also train your own model\n",
    "        model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)\n",
    "        #specity directory, additionally YOLO V5 small model specified (pretrained)\n",
    "        #if you specify custom, path for weights need to be provided\n",
    "        return model\n",
    "    \n",
    "    def score_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Takes a single frame as input, scores frame using the model\n",
    "        :param frame: Input frame in numpy/tuple/list format.\n",
    "        :return: Labels and Coordinates of obj detected by model in that frame\n",
    "        ::\n",
    "        \"\"\"\n",
    "        #take a frame and do a forward pass\n",
    "        self.model.to(self.device) #setting device\n",
    "        if self.input_t == \"Rosbag\":\n",
    "            frame = bridge.imgmsg_to_cv2(bag.read_messages('/davis/left/image_raw')[0].data)\n",
    "        else:\n",
    "            frame = [frame]\n",
    "        #frame = [frame]\n",
    "        results = self.model(frame) #for each frame the boundraies and labels will be stored\n",
    "\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "        #keeps labels/coords of boundary boxes so they can be drawn later\n",
    "        #take all val of first col, and last index in [:, -1]\n",
    "\n",
    "    \n",
    "\n",
    "        return labels, cord\n",
    "    \n",
    "    def class_to_label(self, x):\n",
    "        \"\"\"\n",
    "        For given value of label, return string label\n",
    "        :param x: numeric label\n",
    "        :return: corresponding string label\n",
    "        :r type: string\n",
    "        \"\"\"\n",
    "        return self.classes[int(x)]\n",
    "    \n",
    "    def plot_boxes(self, results, frame):\n",
    "        \"\"\"\n",
    "        Takes a given frame and results as input and then overlays bounding boxes and labels on the frame.\n",
    "        :param results: Contains labels and coords predicted by model on frame.\n",
    "        :param frame: Frame that has been scored.\n",
    "        :return: Frame with bounding boxes and labels overlayed on it\n",
    "        \"\"\"\n",
    "        labels, cord = results\n",
    "        n = len(labels) #number of detected labels\n",
    "        x_shape, y_shape = frame.shape[1], frame.shape[0]\n",
    "\n",
    "        for i in range(n): #running through all the detections\n",
    "            row = cord[i]\n",
    "            if row[4]>=0.2:\n",
    "                x1, y1, x2, y2 = int(row[0]*x_shape), int(row[1]*y_shape), int(row[2]*x_shape), int(row[3]*y_shape)\n",
    "                bgr =  (0, 0, 255) #colour of boundary box, currently red\n",
    "                label = self.class_to_label(labels[i])\n",
    "                confidence = row[4]\n",
    "                text = f\"{label}: {confidence:.2f}\" #label and confidence text to be shown\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), bgr, 2) #draw rectangle around object\n",
    "                cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, bgr, 2) #displaying correspoding label\n",
    "        return frame \n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        This function is called when the class is executed. Runs loop to read video frame by frame and outputs the result to a new file\n",
    "        :return: void\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "        player = self.get_video_from_url()\n",
    "        assert player.isOpened()\n",
    "        x_shape = int(player.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        y_shape = int(player.get(cv2.CAP_PROP_FRAME_HEIGHT)) #output resolution\n",
    "        four_cc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        out = cv2.VideoWriter(self.out_file, four_cc, 60, (x_shape, y_shape))\n",
    "\n",
    "        while True: #as long as you have frames in video\n",
    "            start_time = time() #timer\n",
    "            if self.input_t == \"Rosbag\":\n",
    "                ret, frame = cap.read()\n",
    "            else:\n",
    "                ret, frame = player.read()\n",
    "            #ret, frame = player.read() #load frame from video\n",
    "            if not ret:\n",
    "                break\n",
    "            results = self.score_frame(frame) #get results\n",
    "            frame = self.plot_boxes(results, frame) #plot boxes\n",
    "            \n",
    "            # Display the frame with bounding boxes and labels in real-time\n",
    "            cv2.imshow('Object Detection', frame)\n",
    "            cv2.waitKey(1)  # Wait for a key event (1 millisecond delay)\n",
    "\n",
    "            # Check for 'q' key press to exit the video processing\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            end_time = time()\n",
    "            fps = 1/np.round(end_time-start_time, 3) #calculate fps\n",
    "            print(f\"FPS:{fps}\")\n",
    "            print(x_shape, y_shape)\n",
    "            out.write(frame)\n",
    "        # Release the video capture and close the window\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#create new obj and execute\n",
    "#give video url and output file name\n",
    "\n",
    "detection = ObjectDetection(\"/home/shashank/Downloads/outdoor_day1_data.bag\", \"Rosbag\", \"video_t7.avi\")\n",
    "detection()\n",
    "#choose between 'Local', 'Webcam' and 'YT' for input\n",
    "#either give URL or path for YT and Local respectively\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     cv_image \u001b[39m=\u001b[39m bridge\u001b[39m.\u001b[39mimgmsg_to_cv2(msg, desired_encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbgr8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mImage\u001b[39m\u001b[39m'\u001b[39m, cv_image)\n\u001b[0;32m---> 11\u001b[0m     cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     13\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m     14\u001b[0m bag\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import rosbag\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "\n",
    "bag = rosbag.Bag('/home/shashank/Downloads/outdoor_day1_data.bag')\n",
    "bridge = CvBridge()\n",
    "\n",
    "for topic, msg, t in bag.read_messages(topics=['/davis/left/image_raw']):\n",
    "    cv_image = bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n",
    "    cv2.imshow('Image', cv_image)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "bag.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rosbag\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "\n",
    "bag = rosbag.Bag('/home/shashank/Downloads/outdoor_day1_data.bag')\n",
    "bridge = CvBridge()\n",
    "\n",
    "# Create a VideoWriter object to save the video\n",
    "output_file = 'output_video.avi'\n",
    "output_fps = 30  # Set the desired output frames per second (FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "video_writer = cv2.VideoWriter(output_file, fourcc, output_fps, (640, 480))  # Update the resolution if needed\n",
    "\n",
    "for topic, msg, t in bag.read_messages(topics=['/davis/left/image_raw']):\n",
    "    cv_image = bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n",
    "    \n",
    "    # Write the frame to the video file\n",
    "    video_writer.write(cv_image)\n",
    "    \n",
    "    cv2.imshow('Image', cv_image)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "bag.close()\n",
    "video_writer.release()  # Release the video writer when done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Loop over the messages in the bag.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mwhile\u001b[39;00m cap\u001b[39m.\u001b[39misOpened():\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m     \u001b[39m# Read the next image from the bag.\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m     24\u001b[0m     \u001b[39m# Write the image to the virtual webcam.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     webcam\u001b[39m.\u001b[39mwrite(frame)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import rosbag\n",
    "from cv_bridge import CvBridge\n",
    "\n",
    "bag = rosbag.Bag('/home/shashank/Downloads/outdoor_day1_data.bag')\n",
    "bridge = CvBridge()\n",
    "\n",
    "# Create a cv2.VideoCapture object and pass the rosbag file to the constructor.\n",
    "cap = cv2.VideoCapture(\"/dev/video0\")\n",
    "\n",
    "# Set the camera settings.\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# Create a virtual webcam.\n",
    "webcam = cv2.VideoWriter(\"/dev/video1\", cv2.VideoWriter_fourcc('M','J','P','G'), 30, (640, 480))\n",
    "\n",
    "# Loop over the messages in the bag.\n",
    "while cap.isOpened():\n",
    "\n",
    "    # Read the next image from the bag.\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Write the image to the virtual webcam.\n",
    "    webcam.write(frame)\n",
    "\n",
    "    # Display the image on the screen.\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "\n",
    "    # Wait for a key press.\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    # If the user presses ESC, close the window.\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "# Close the rosbag file.\n",
    "bag.close()\n",
    "\n",
    "# Close the virtual webcam.\n",
    "webcam.release()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/shashank/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-5-20 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (Quadro T1000 with Max-Q Design, 3912MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /home/shashank/.local/lib/python3.8/site-packages/requirements.txt not found, check failed.\n",
      "Device Used: cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39m# Create an instance of the ObjectDetection class and execute\u001b[39;00m\n\u001b[1;32m    107\u001b[0m detection \u001b[39m=\u001b[39m ObjectDetection(\u001b[39m\"\u001b[39m\u001b[39m/home/shashank/Downloads/outdoor_day1_data.bag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moutput_video.avi\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m detection()\n",
      "Cell \u001b[0;32mIn[4], line 92\u001b[0m, in \u001b[0;36mObjectDetection.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m topic, msg, t \u001b[39min\u001b[39;00m bag\u001b[39m.\u001b[39mread_messages(topics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m/davis/left/image_raw\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     90\u001b[0m     cv_image \u001b[39m=\u001b[39m bridge\u001b[39m.\u001b[39mimgmsg_to_cv2(msg, desired_encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbgr8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscore_frame(cv_image)\n\u001b[1;32m     93\u001b[0m     cv_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot_boxes(results, cv_image)\n\u001b[1;32m     95\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mObject Detection\u001b[39m\u001b[39m'\u001b[39m, cv_image)\n",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mObjectDetection.score_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     40\u001b[0m frame \u001b[39m=\u001b[39m [frame]\n\u001b[0;32m---> 41\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(frame)\n\u001b[1;32m     43\u001b[0m labels, cord \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mxyxyn[\u001b[39m0\u001b[39m][:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], results\u001b[39m.\u001b[39mxyxyn[\u001b[39m0\u001b[39m][:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m labels, cord\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:705\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(autocast):\n\u001b[1;32m    703\u001b[0m     \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[39mwith\u001b[39;00m dt[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 705\u001b[0m         y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, augment\u001b[39m=\u001b[39;49maugment)  \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[39m# Post-process\u001b[39;00m\n\u001b[1;32m    708\u001b[0m     \u001b[39mwith\u001b[39;00m dt[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:515\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    512\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im, augment\u001b[39m=\u001b[39maugment, visualize\u001b[39m=\u001b[39mvisualize) \u001b[39mif\u001b[39;00m augment \u001b[39mor\u001b[39;00m visualize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im)\n\u001b[1;32m    516\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:209\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m    208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_augment(x)  \u001b[39m# augmented inference, None\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_once(x, profile, visualize)\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:121\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 121\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m    122\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:76\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m             xy \u001b[39m=\u001b[39m (xy \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid[i]) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride[i]  \u001b[39m# xy\u001b[39;00m\n\u001b[1;32m     75\u001b[0m             wh \u001b[39m=\u001b[39m (wh \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manchor_grid[i]  \u001b[39m# wh\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m             y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((xy, wh, conf), \u001b[39m4\u001b[39;49m)\n\u001b[1;32m     77\u001b[0m         z\u001b[39m.\u001b[39mappend(y\u001b[39m.\u001b[39mview(bs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mna \u001b[39m*\u001b[39m nx \u001b[39m*\u001b[39m ny, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno))\n\u001b[1;32m     79\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39melse\u001b[39;00m (torch\u001b[39m.\u001b[39mcat(z, \u001b[39m1\u001b[39m),) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexport \u001b[39melse\u001b[39;00m (torch\u001b[39m.\u001b[39mcat(z, \u001b[39m1\u001b[39m), x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "import rosbag\n",
    "\n",
    "class ObjectDetection:\n",
    "    \"\"\"\n",
    "    Implements the YOLO V5 Model on ROS bag images using OpenCV\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bag_file, out_file):\n",
    "        \"\"\"\n",
    "        Initializes the class with the ROS bag file and the output file\n",
    "        :param bag_file: Path to the ROS bag file\n",
    "        :param out_file: A valid output file name.\n",
    "        \"\"\"\n",
    "        self.bag_file = bag_file\n",
    "        self.out_file = out_file\n",
    "        self.model = self.load_model()\n",
    "        self.classes = self.model.names\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(\"Device Used:\", self.device)\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads YOLO V5 Model from PyTorch\n",
    "        :return: Trained model from PyTorch\n",
    "        \"\"\"\n",
    "        model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)\n",
    "        return model\n",
    "\n",
    "    def score_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Takes a single frame as input and scores the frame using the model\n",
    "        :param frame: Input frame in numpy/tuple/list format.\n",
    "        :return: Labels and coordinates of objects detected by the model in that frame\n",
    "        \"\"\"\n",
    "        self.model.to(self.device)\n",
    "        frame = [frame]\n",
    "        results = self.model(frame)\n",
    "\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "        return labels, cord\n",
    "\n",
    "    def class_to_label(self, x):\n",
    "        \"\"\"\n",
    "        For a given value of label, returns the corresponding string label\n",
    "        :param x: Numeric label\n",
    "        :return: Corresponding string label\n",
    "        \"\"\"\n",
    "        return self.classes[int(x)]\n",
    "\n",
    "    def plot_boxes(self, results, frame):\n",
    "        \"\"\"\n",
    "        Takes a given frame and results as input and overlays bounding boxes and labels on the frame.\n",
    "        :param results: Contains labels and coordinates predicted by the model on the frame.\n",
    "        :param frame: Frame that has been scored.\n",
    "        :return: Frame with bounding boxes and labels overlaid on it\n",
    "        \"\"\"\n",
    "        labels, cord = results\n",
    "        n = len(labels)\n",
    "        x_shape, y_shape = frame.shape[1], frame.shape[0]\n",
    "\n",
    "        for i in range(n):\n",
    "            row = cord[i]\n",
    "            if row[4] >= 0.2:\n",
    "                x1, y1, x2, y2 = int(row[0] * x_shape), int(row[1] * y_shape), int(row[2] * x_shape), int(row[3] * y_shape)\n",
    "                bgr = (0, 0, 255)\n",
    "                label = self.class_to_label(labels[i])\n",
    "                confidence = row[4]\n",
    "                text = f\"{label}: {confidence:.2f}\"\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), bgr, 2)\n",
    "                cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, bgr, 2)\n",
    "        return frame\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        This function is called when the class is executed. Reads images from the ROS bag file and outputs the result to a new file.\n",
    "        \"\"\"\n",
    "        bag = rosbag.Bag(self.bag_file)\n",
    "        bridge = CvBridge()\n",
    "\n",
    "        # Create a VideoWriter object to save the video\n",
    "        output_fps = 30  # Set the desired output frames per second (FPS)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        video_writer = cv2.VideoWriter(self.out_file, fourcc, output_fps, (1280, 720))  # Update the resolution if needed\n",
    "\n",
    "        for topic, msg, t in bag.read_messages(topics=['/davis/left/image_raw']):\n",
    "            cv_image = bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n",
    "\n",
    "            results = self.score_frame(cv_image)\n",
    "            cv_image = self.plot_boxes(results, cv_image)\n",
    "\n",
    "            cv2.imshow('Object Detection', cv_image)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "            # Write the frame to the video file\n",
    "            video_writer.write(cv_image)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        bag.close()\n",
    "        video_writer.release()\n",
    "\n",
    "\n",
    "# Create an instance of the ObjectDetection class and execute\n",
    "detection = ObjectDetection(\"/home/shashank/Downloads/outdoor_day1_data.bag\", \"output_video.avi\")\n",
    "detection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/shashank/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-5-20 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (Quadro T1000 with Max-Q Design, 3912MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /home/shashank/.local/lib/python3.8/site-packages/requirements.txt not found, check failed.\n",
      "\n",
      " \n",
      "Device Used (if its not cuda gl) cuda\n",
      "Opening Webcam\n",
      "FPS:1.5552099533437014\n",
      "1280 720\n",
      "FPS:43.47826086956522\n",
      "1280 720\n",
      "FPS:10.416666666666666\n",
      "1280 720\n",
      "FPS:10.204081632653061\n",
      "1280 720\n",
      "FPS:12.345679012345679\n",
      "1280 720\n",
      "FPS:10.526315789473685\n",
      "1280 720\n",
      "FPS:13.333333333333334\n",
      "1280 720\n",
      "FPS:10.416666666666666\n",
      "1280 720\n",
      "FPS:10.416666666666666\n",
      "1280 720\n",
      "FPS:11.76470588235294\n",
      "1280 720\n",
      "FPS:10.638297872340425\n",
      "1280 720\n",
      "FPS:12.345679012345679\n",
      "1280 720\n",
      "FPS:13.333333333333334\n",
      "1280 720\n",
      "FPS:11.904761904761903\n",
      "1280 720\n",
      "FPS:13.698630136986303\n",
      "1280 720\n",
      "FPS:11.49425287356322\n",
      "1280 720\n",
      "FPS:11.11111111111111\n",
      "1280 720\n",
      "FPS:11.904761904761903\n",
      "1280 720\n",
      "FPS:10.526315789473685\n",
      "1280 720\n",
      "FPS:12.820512820512821\n",
      "1280 720\n",
      "FPS:10.1010101010101\n",
      "1280 720\n",
      "FPS:11.363636363636365\n",
      "1280 720\n",
      "FPS:9.900990099009901\n",
      "1280 720\n",
      "FPS:11.904761904761903\n",
      "1280 720\n",
      "FPS:14.285714285714285\n",
      "1280 720\n",
      "FPS:9.523809523809524\n",
      "1280 720\n",
      "FPS:14.084507042253522\n",
      "1280 720\n",
      "FPS:14.925373134328357\n",
      "1280 720\n",
      "FPS:8.928571428571429\n",
      "1280 720\n",
      "FPS:14.492753623188404\n",
      "1280 720\n",
      "FPS:11.627906976744187\n",
      "1280 720\n",
      "FPS:9.523809523809524\n",
      "1280 720\n",
      "FPS:13.157894736842106\n",
      "1280 720\n",
      "FPS:12.987012987012987\n",
      "1280 720\n",
      "FPS:9.523809523809524\n",
      "1280 720\n",
      "FPS:10.869565217391305\n",
      "1280 720\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 186\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m#create new obj and execute\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m#give video url and output file name\u001b[39;00m\n\u001b[1;32m    185\u001b[0m detection \u001b[39m=\u001b[39m ObjectDetection(\u001b[39m\"\u001b[39m\u001b[39m/home/shashank/Downloads/outdoor_day1_data.bag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mWebcam\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvideo_t7.avi\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 186\u001b[0m detection()\n\u001b[1;32m    187\u001b[0m \u001b[39m#choose between 'Local', 'Webcam' and 'YT' for input\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m#either give URL or path for YT and Local respectively\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 159\u001b[0m, in \u001b[0;36mObjectDetection.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m: \u001b[39m#as long as you have frames in video\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     start_time \u001b[39m=\u001b[39m time() \u001b[39m#timer\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     ret, frame \u001b[39m=\u001b[39m player\u001b[39m.\u001b[39;49mread() \u001b[39m#load frame from video\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ret:\n\u001b[1;32m    161\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pafy #take videos from yt and pass to model\n",
    "from time import time\n",
    "import rosbag\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "\n",
    "#note: base code developed using: https://www.youtube.com/watch?v=3wdqO_vYMpA&t=0s\n",
    "\n",
    "class ObjectDetection:\n",
    "    \"\"\"\n",
    "    Implements the YOLO V5 Model on a YT video, webcam or local file using OpenCV \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url, inp_typ, out_file):\n",
    "        \"\"\"\n",
    "        Initialises the class with the YT Url and the Output File\n",
    "        :param url: A valid YT URL OR Local file location\n",
    "        :paral inp_typ: User defined either 'Webcam', 'Local' or 'YT'\n",
    "        :out_file: A valid output file name.\n",
    "        :r type: None\n",
    "        \"\"\"\n",
    "        #initilising attributes\n",
    "        self.input_t = inp_typ \n",
    "        self._URL = url  #can be YT url or local file path\n",
    "        self.model = self.load_model()\n",
    "        self.classes = self.model.names\n",
    "        self.out_file = out_file\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu :(' #checks if cuda is available and uses it if it is\n",
    "        print(\"\\n \\nDevice Used (if its not cuda gl)\", self.device)\n",
    "\n",
    "    def get_video_from_url(self):\n",
    "        \"\"\"\n",
    "        Generates video streaming object. Frame by frame extraction will be done in order to make predictions\n",
    "        :return: openCV2 video capture object, with lowest quality frame available for video\n",
    "        \"\"\"\n",
    "        #distingish between local file and YT input\n",
    "        \n",
    "        \n",
    "        if self.input_t == \"Webcam\":\n",
    "            print(\"Opening Webcam\")\n",
    "            cap = cv2.VideoCapture(0)\n",
    "            # Set resolution of input frames to 640x480\n",
    "            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "            # Set frame rate of input frames to 30 frames per second\n",
    "            cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "            return cap\n",
    "        \n",
    "        elif self.input_t == \"Local\":\n",
    "            print(\"Loading local video file\")\n",
    "            input_file = self._URL #test for mp4\n",
    "            cap = cv2.VideoCapture(input_file)\n",
    "            # Set resolution of input frames to 640x480\n",
    "            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) #doesnt seem to work yet, check\n",
    "\n",
    "            # Set frame rate of input rames to 30 frames per second\n",
    "            cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "            return cap\n",
    "\n",
    "        elif self.input_t == \"YT\":\n",
    "            print(\"Loading YT Video\")\n",
    "            play = pafy.new(self._URL).streams[-1]\n",
    "            input_file = play.url\n",
    "            cap = cv2.VideoCapture(input_file)\n",
    "            # Set resolution of input frames to 640x480\n",
    "            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 5)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 5)\n",
    "\n",
    "            # Set frame rate of input frames to 30 frames per second\n",
    "            cap.set(cv2.CAP_PROP_FPS, 100)\n",
    "            return cap\n",
    "        \n",
    "\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads YOLO V5 Model from PyTorch\n",
    "        :return: Train model from PyTorch\n",
    "        \"\"\"\n",
    "        #you can also train your own model\n",
    "        model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)\n",
    "        #specity directory, additionally YOLO V5 small model specified (pretrained)\n",
    "        #if you specify custom, path for weights need to be provided\n",
    "        return model\n",
    "    \n",
    "    def score_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Takes a single frame as input, scores frame using the model\n",
    "        :param frame: Input frame in numpy/tuple/list format.\n",
    "        :return: Labels and Coordinates of obj detected by model in that frame\n",
    "        ::\n",
    "        \"\"\"\n",
    "        #take a frame and do a forward pass\n",
    "        self.model.to(self.device) #setting device\n",
    "        frame = [frame]\n",
    "        results = self.model(frame) #for each frame the boundraies and labels will be stored\n",
    "\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "        #keeps labels/coords of boundary boxes so they can be drawn later\n",
    "        #take all val of first col, and last index in [:, -1]\n",
    "\n",
    "    \n",
    "\n",
    "        return labels, cord\n",
    "    \n",
    "    def class_to_label(self, x):\n",
    "        \"\"\"\n",
    "        For given value of label, return string label\n",
    "        :param x: numeric label\n",
    "        :return: corresponding string label\n",
    "        :r type: string\n",
    "        \"\"\"\n",
    "        return self.classes[int(x)]\n",
    "    \n",
    "    def plot_boxes(self, results, frame):\n",
    "        \"\"\"\n",
    "        Takes a given frame and results as input and then overlays bounding boxes and labels on the frame.\n",
    "        :param results: Contains labels and coords predicted by model on frame.\n",
    "        :param frame: Frame that has been scored.\n",
    "        :return: Frame with bounding boxes and labels overlayed on it\n",
    "        \"\"\"\n",
    "        labels, cord = results\n",
    "        n = len(labels) #number of detected labels\n",
    "        x_shape, y_shape = frame.shape[1], frame.shape[0]\n",
    "\n",
    "        for i in range(n): #running through all the detections\n",
    "            row = cord[i]\n",
    "            if row[4]>=0.2:\n",
    "                x1, y1, x2, y2 = int(row[0]*x_shape), int(row[1]*y_shape), int(row[2]*x_shape), int(row[3]*y_shape)\n",
    "                bgr =  (0, 0, 255) #colour of boundary box, currently red\n",
    "                label = self.class_to_label(labels[i])\n",
    "                confidence = row[4]\n",
    "                text = f\"{label}: {confidence:.2f}\" #label and confidence text to be shown\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), bgr, 2) #draw rectangle around object\n",
    "                cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, bgr, 2) #displaying correspoding label\n",
    "        return frame \n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        This function is called when the class is executed. Runs loop to read video frame by frame and outputs the result to a new file\n",
    "        :return: void\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "        player = self.get_video_from_url()\n",
    "        assert player.isOpened()\n",
    "        x_shape = int(player.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        y_shape = int(player.get(cv2.CAP_PROP_FRAME_HEIGHT)) #output resolution\n",
    "        four_cc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        out = cv2.VideoWriter(self.out_file, four_cc, 60, (x_shape, y_shape))\n",
    "\n",
    "        while True: #as long as you have frames in video\n",
    "            start_time = time() #timer\n",
    "            ret, frame = player.read() #load frame from video\n",
    "            if not ret:\n",
    "                break\n",
    "            results = self.score_frame(frame) #get results\n",
    "            frame = self.plot_boxes(results, frame) #plot boxes\n",
    "            \n",
    "            # Display the frame with bounding boxes and labels in real-time\n",
    "            cv2.imshow('Object Detection', frame)\n",
    "            cv2.waitKey(1)  # Wait for a key event (1 millisecond delay)\n",
    "\n",
    "            # Check for 'q' key press to exit the video processing\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            end_time = time()\n",
    "            fps = 1/np.round(end_time-start_time, 3) #calculate fps\n",
    "            print(f\"FPS:{fps}\")\n",
    "            print(x_shape, y_shape)\n",
    "            out.write(frame)\n",
    "        # Release the video capture and close the window\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#create new obj and execute\n",
    "#give video url and output file name\n",
    "\n",
    "detection = ObjectDetection(\"/home/shashank/Downloads/outdoor_day1_data.bag\", \"Webcam\", \"video_t7.avi\")\n",
    "detection()\n",
    "#choose between 'Local', 'Webcam' and 'YT' for input\n",
    "#either give URL or path for YT and Local respectively\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
